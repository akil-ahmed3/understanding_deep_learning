{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 9000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "images = np.load(\"images.npy\")\n",
    "labels = np.load(\"labels.npy\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    images, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# X_train = np.load(\"images_train.npy\")\n",
    "# y_train = np.load(\"labels_train.npy\")\n",
    "# X_test = np.load(\"images_test.npy\")\n",
    "# y_test = np.load(\"labels_test.npy\")\n",
    "\n",
    "X_train_flatten = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
    "y_train = y_train.reshape(1, y_train.shape[0])\n",
    "y_test = y_test.reshape(1, y_test.shape[0])\n",
    "\n",
    "train_set_x = X_train_flatten / 255.\n",
    "test_set_x = X_test_flatten / 255.\n",
    "\n",
    "print(train_set_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros([dim, 1], dtype=\"float64\")\n",
    "    b = 0.0\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -(np.sum(Y * np.log(A) + (1.0 - Y) * np.log(1.0 - A))) / m\n",
    "\n",
    "    dw = np.dot(X, (A - Y).T) / m\n",
    "    db = np.sum(A - Y) / m\n",
    "\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return grads, cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(w, b, X, Y, num_iterations=100, learning_rate=0.009):\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] > 0 and A[0, i] < 0.5:\n",
    "            Y_prediction[0, i] = 0\n",
    "        else:\n",
    "            Y_prediction[0, i] = 1\n",
    "\n",
    "    return Y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5):\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    params, grads, costs = gradient_decent(\n",
    "        w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(\n",
    "        100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(\n",
    "        100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    c.append(costs)\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test,\n",
    "         \"Y_prediction_train\": Y_prediction_train,\n",
    "         \"w\": w,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.668503\n",
      "Cost after iteration 200: 0.663777\n",
      "Cost after iteration 300: 0.661196\n",
      "Cost after iteration 400: 0.659372\n",
      "Cost after iteration 500: 0.657910\n",
      "Cost after iteration 600: 0.656658\n",
      "Cost after iteration 700: 0.655542\n",
      "Cost after iteration 800: 0.654524\n",
      "Cost after iteration 900: 0.653581\n",
      "Cost after iteration 1000: 0.652698\n",
      "Cost after iteration 1100: 0.651865\n",
      "Cost after iteration 1200: 0.651075\n",
      "Cost after iteration 1300: 0.650323\n",
      "Cost after iteration 1400: 0.649604\n",
      "Cost after iteration 1500: 0.648915\n",
      "Cost after iteration 1600: 0.648254\n",
      "Cost after iteration 1700: 0.647618\n",
      "Cost after iteration 1800: 0.647006\n",
      "Cost after iteration 1900: 0.646415\n",
      "Cost after iteration 2000: 0.645844\n",
      "Cost after iteration 2100: 0.645292\n",
      "Cost after iteration 2200: 0.644757\n",
      "Cost after iteration 2300: 0.644239\n",
      "Cost after iteration 2400: 0.643737\n",
      "Cost after iteration 2500: 0.643249\n",
      "Cost after iteration 2600: 0.642775\n",
      "Cost after iteration 2700: 0.642314\n",
      "Cost after iteration 2800: 0.641866\n",
      "Cost after iteration 2900: 0.641429\n",
      "Cost after iteration 3000: 0.641004\n",
      "Cost after iteration 3100: 0.640589\n",
      "Cost after iteration 3200: 0.640184\n",
      "Cost after iteration 3300: 0.639789\n",
      "Cost after iteration 3400: 0.639404\n",
      "Cost after iteration 3500: 0.639027\n",
      "Cost after iteration 3600: 0.638659\n",
      "Cost after iteration 3700: 0.638298\n",
      "Cost after iteration 3800: 0.637946\n",
      "Cost after iteration 3900: 0.637601\n",
      "Cost after iteration 4000: 0.637263\n",
      "Cost after iteration 4100: 0.636932\n",
      "Cost after iteration 4200: 0.636607\n",
      "Cost after iteration 4300: 0.636289\n",
      "Cost after iteration 4400: 0.635977\n",
      "Cost after iteration 4500: 0.635671\n",
      "Cost after iteration 4600: 0.635370\n",
      "Cost after iteration 4700: 0.635075\n",
      "Cost after iteration 4800: 0.634785\n",
      "Cost after iteration 4900: 0.634500\n",
      "Cost after iteration 5000: 0.634220\n",
      "Cost after iteration 5100: 0.633945\n",
      "Cost after iteration 5200: 0.633675\n",
      "Cost after iteration 5300: 0.633408\n",
      "Cost after iteration 5400: 0.633146\n",
      "Cost after iteration 5500: 0.632889\n",
      "Cost after iteration 5600: 0.632635\n",
      "Cost after iteration 5700: 0.632385\n",
      "Cost after iteration 5800: 0.632139\n",
      "Cost after iteration 5900: 0.631897\n",
      "Cost after iteration 6000: 0.631658\n",
      "Cost after iteration 6100: 0.631422\n",
      "Cost after iteration 6200: 0.631190\n",
      "Cost after iteration 6300: 0.630961\n",
      "Cost after iteration 6400: 0.630736\n",
      "Cost after iteration 6500: 0.630513\n",
      "Cost after iteration 6600: 0.630293\n",
      "Cost after iteration 6700: 0.630076\n",
      "Cost after iteration 6800: 0.629863\n",
      "Cost after iteration 6900: 0.629651\n",
      "Cost after iteration 7000: 0.629443\n",
      "Cost after iteration 7100: 0.629237\n",
      "Cost after iteration 7200: 0.629034\n",
      "Cost after iteration 7300: 0.628833\n",
      "Cost after iteration 7400: 0.628634\n",
      "Cost after iteration 7500: 0.628438\n",
      "Cost after iteration 7600: 0.628244\n",
      "Cost after iteration 7700: 0.628053\n",
      "Cost after iteration 7800: 0.627863\n",
      "Cost after iteration 7900: 0.627676\n",
      "Cost after iteration 8000: 0.627491\n",
      "Cost after iteration 8100: 0.627308\n",
      "Cost after iteration 8200: 0.627126\n",
      "Cost after iteration 8300: 0.626947\n",
      "Cost after iteration 8400: 0.626770\n",
      "Cost after iteration 8500: 0.626594\n",
      "Cost after iteration 8600: 0.626421\n",
      "Cost after iteration 8700: 0.626249\n",
      "Cost after iteration 8800: 0.626079\n",
      "Cost after iteration 8900: 0.625910\n",
      "Cost after iteration 9000: 0.625744\n",
      "Cost after iteration 9100: 0.625579\n",
      "Cost after iteration 9200: 0.625415\n",
      "Cost after iteration 9300: 0.625253\n",
      "Cost after iteration 9400: 0.625093\n",
      "Cost after iteration 9500: 0.624934\n",
      "Cost after iteration 9600: 0.624776\n",
      "Cost after iteration 9700: 0.624620\n",
      "Cost after iteration 9800: 0.624466\n",
      "Cost after iteration 9900: 0.624313\n",
      "train accuracy: 65.91111111111111 %\n",
      "test accuracy: 58.0 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = model(train_set_x, y_train, test_set_x, y_test, num_iterations=10000, learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0, Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac1f22b8b8adbf3b42f127373cca76cbcc6aab5eb09b52700604e780582ff129"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
